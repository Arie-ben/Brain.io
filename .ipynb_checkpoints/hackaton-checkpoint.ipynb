{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from operator import itemgetter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def is_url_valid(url):\n",
    "    if url[:4] != \"http\":\n",
    "        return False\n",
    "    return True\n",
    "    \n",
    "\n",
    "def domain_without_www(domain):\n",
    "    if domain[:4] == \"www.\":\n",
    "        return domain[4:]\n",
    "    return domain\n",
    "\n",
    "\n",
    "def filter_rows(df):\n",
    "    df['is_valid'] = df.url.apply(is_url_valid)\n",
    "    df = df[df['is_valid'] == True]\n",
    "    df = df.drop(['is_valid'], axis=1)\n",
    "    df['domain'] = df.domain.apply(domain_without_www)\n",
    "    df = df.dropna()\n",
    "    df = df.sort_values('visit_time_ms', ascending=False)\n",
    "    return df.reset_index().drop('index', axis=1)\n",
    "\n",
    "\n",
    "def fuck_of_this_domain(df, domains):\n",
    "    for domain in domains:\n",
    "        df = df.loc[df['domain'] != domain]\n",
    "    return df.reset_index().drop('index', axis=1)\n",
    "    \n",
    "\n",
    "def clean_date(df):   \n",
    "    df.day = [re.sub('[-:]', '/', df.day[i]) for i in range(len(df))]\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        try:\n",
    "            df.day[i] = pd.to_datetime(df.day[i] + '-'+ df.time[i])\n",
    "        except ValueError:\n",
    "            df.day[i] = None\n",
    "    return filter_rows(df.drop('time', axis=1))\n",
    "\n",
    "\n",
    "def find_indexes(df, url):\n",
    "    return df.loc[df['url'] ==  url].index.tolist()\n",
    "\n",
    "\n",
    "def replace_same_url(df, url, indexes):\n",
    "    index = 0\n",
    "    while index < len(indexes):\n",
    "        if df.url[indexes[index]] == url:\n",
    "            indexes.pop(index)\n",
    "            last_i = indexes[-1] + 1\n",
    "            while df.url[last_i] == url and last_i != df.index.max():\n",
    "                last_i += 1\n",
    "            indexes += [last_i]\n",
    "        else:\n",
    "            index += 1\n",
    "    return indexes\n",
    "\n",
    "\n",
    "def weighted_urls(df, url, previous_url, NUM_OF_NEXT=5, NUM_OF_PREVIOUS=2):\n",
    "    indexes_after = [range(element - NUM_OF_NEXT, element) for element in find_indexes(df, url) if element - NUM_OF_NEXT > 0]\n",
    "    for index in range(len(indexes_after)):\n",
    "        indexes_after[index] = replace_same_url(df, url, indexes_after[index])\n",
    "    indexes_before = [range(element + 1, element + 1 + NUM_OF_PREVIOUS) for element in find_indexes(df, url)]\n",
    "    if indexes_after:\n",
    "        if indexes_after[0] == []:\n",
    "            indexes_after.pop(0)\n",
    "\n",
    "    weights = [float(1)/2**i for i in range(NUM_OF_NEXT)]\n",
    "    urls_before = [df.url[i].values  for i in indexes_before]\n",
    "    urls_after = [df.url[i].values  for i in indexes_after]\n",
    "    urls_weighted = []\n",
    "\n",
    "    for j in range(len(urls_after)):\n",
    "        weights = [float(1)/2**(NUM_OF_NEXT - i -1) for i in range(NUM_OF_NEXT)]\n",
    "        coef = 1\n",
    "        i = 0\n",
    "        while i < NUM_OF_PREVIOUS and previous_url[i] == urls_before[j][i]:\n",
    "            weights = [weight*2 for weight in weights]\n",
    "            i += 1\n",
    "        urls_weighted.append([(urls_after[j][k], weights[k]) for k in range(len(weights))])            \n",
    "    return urls_weighted\n",
    "\n",
    "\n",
    "def get_weigths(df, url, previous_url):\n",
    "    \n",
    "    weigth_dic = {}\n",
    "    for url_list in weighted_urls(df, url, previous_url):\n",
    "        for url in url_list:\n",
    "            if url[0] in weigth_dic:\n",
    "                weigth_dic[url[0]] += url[1]\n",
    "            else:\n",
    "                weigth_dic[url[0]] = url[1]\n",
    "\n",
    "    return sorted(weigth_dic.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "\n",
    "def is_in_time(df, index, threshold):\n",
    "    if df.day[index-1] - df.day[index] < pd.Timedelta(threshold, 's') :\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_int(number):\n",
    "    if not isinstance(number, float) and \",\" in number:\n",
    "        number = number.replace(\",\", \".\")\n",
    "    return float(number)\n",
    "\n",
    "\n",
    "def delta(df):\n",
    "    df.visit_time_ms = df.visit_time_ms.apply(to_int)\n",
    "    df = df.sort_values('visit_time_ms', ascending = False)\n",
    "    df['delta'] = [df.visit_time_ms[index] - df.visit_time_ms[index + 1] for index in range(df.shape[0] - 1)] + [0]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def treat_data(df_path, filtered_domains):\n",
    "    df = pd.read_csv(df_path, delimiter=\";\", header=None)\n",
    "    df.columns = (['url', 'domain', 'root domain', 'visit_time_ms', 'visit_time_str', 'day of the week', 'transition_type', 'page title'])\n",
    "    df = df[['url', 'domain', 'visit_time_ms', 'visit_time_str', 'transition_type']]\n",
    "    df = filter_rows(df)\n",
    "    df = fuck_of_this_domain(df, filtered_domains)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_other_databases(folder_path, filtered_domains):\n",
    "    df_list = []\n",
    "    for df in os.listdir(folder_path):\n",
    "        if os.path.basename(df)[0] != \".\":\n",
    "            df_list.append(treat_data(folder_path + os.path.basename(df), filtered_domains))\n",
    "    return df_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_domains = (['whatsapp.com','web.whatsapp.com', 'twitter.com', 'linkedin.com',\n",
    "                     'google.co.il','fr-fr.messenger.com','youtube.com','facebook.com', 'localhost',\n",
    "                     'plus.google.com','google.fr', 'mail.google.com', 'google.com','messenger.com',\n",
    "                     'listenonrepeat.com', 'drive.google.com', 'docs.google.com', 'calendar.google.com',\n",
    "                     'chrome.google.com', 'gmail.com', 'lefigaro.fr'])\n",
    "df_path = 'databases/antho_secrets.csv'\n",
    "folder_path = 'databases/other_databases/'\n",
    "df = treat_data(df_path, filtered_domains)\n",
    "df_list = get_other_databases(folder_path, filtered_domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "arg2, arg3 = 'http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html', [ \"\", \"\"]\n",
    "\n",
    "\n",
    "def sum_score(df_list, arg2, arg3):\n",
    "    score_dic = {}\n",
    "    for dataframe in df_list:\n",
    "        for i in get_weigths(dataframe, arg2, arg3):\n",
    "            if i[0] in score_dic:\n",
    "                score_dic[i[0]] += i[1]\n",
    "            else:\n",
    "                score_dic[i[0]] = i[1]\n",
    "\n",
    "    return sorted(score_dic.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "score_dic = sum_score(df_list, arg2, arg3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('http://stackoverflow.com/questions/29438265/stratified-train-test-split-in-scikit-learn', 2.5), ('http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html', 1.6875), ('http://pythoncentral.io/introduction-to-tweepy-twitter-for-python/', 1.5625), ('http://scikit-learn.org/stable/install.html#installation-instructions', 1.1875), ('http://stackoverflow.com/questions/14661701/how-to-drop-a-list-of-rows-from-pandas-dataframe', 1.0)]\n"
     ]
    }
   ],
   "source": [
    "print score_dic[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('http://stackoverflow.com/questions/29438265/stratified-train-test-split-in-scikit-learn', 1.5)\n",
      "('http://stackoverflow.com/questions/20076195/what-is-the-most-efficient-way-of-counting-occurrences-in-pandas', 1.0)\n",
      "('http://datascience.stackexchange.com/questions/2368/machine-learning-features-engineering-from-date-time-data', 1.0)\n",
      "('http://stackoverflow.com/questions/25039626/find-numeric-columns-in-pandas-python', 1.0)\n",
      "('http://stackoverflow.com/questions/13411544/delete-column-from-pandas-dataframe', 1.0)\n",
      "('http://stackoverflow.com/questions/25146121/extracting-just-month-and-year-from-pandas-datetime-column-python', 0.5625)\n",
      "('http://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html', 0.5)\n",
      "('http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html', 0.5)\n",
      "('https://fr-fr.facebook.com/', 0.5)\n",
      "('https://www.quora.com/How-can-I-hide-mouse-cursor', 0.375)\n",
      "('http://stackoverflow.com/questions/34842405/parameter-stratify-from-method-train-test-split-scikit-learn', 0.25)\n",
      "('http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html', 0.25)\n",
      "('http://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/', 0.25)\n",
      "('http://stackoverflow.com/questions/15509345/python-datetime-extract-double-digit-month-and-day-values', 0.25)\n",
      "('https://www.kaggle.com/juliencs/house-prices-advanced-regression-techniques/a-study-on-regression-applied-to-the-ames-dataset/code', 0.1875)\n",
      "('http://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.GaussianNB.html', 0.125)\n",
      "('http://stackoverflow.com/questions/16729483/converting-strings-to-floats-in-a-dataframe', 0.125)\n",
      "('http://stackoverflow.com/questions/16266019/python-pandas-group-datetime-column-into-hour-and-minute-aggregations', 0.125)\n",
      "('http://devcenter.wintellect.com/jrobbins/a-cool-windbg-sos-hidden-feature', 0.0625)\n",
      "('https://www.quora.com/What-is-the-decision-boundary-for-Naive-Bayes', 0.0625)\n",
      "('http://stackoverflow.com/questions/24037507/converting-string-objects-to-int-float-using-pandas', 0.0625)\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "('http://pythoncentral.io/introduction-to-tweepy-twitter-for-python/', 1.5625)\n",
      "('http://pandas.pydata.org/pandas-docs/stable/merging.html', 1.0)\n",
      "('http://stackoverflow.com/questions/12207326/pandas-frequency-table-for-a-single-variable', 1.0)\n",
      "('http://hive.itcapp.com/messages/thread/19637', 1.0)\n",
      "('http://scikit-learn.org/stable/modules/neighbors.html', 0.5)\n",
      "('https://classroom.udacity.com/courses/ud810/lessons/3461518740/concepts/34384590920923', 0.5)\n",
      "('http://stackoverflow.com/questions/29438265/stratified-train-test-split-in-scikit-learn', 0.5)\n",
      "('https://classroom.udacity.com/courses/ud810/lessons/3461518740/concepts/34384590920923#', 0.375)\n",
      "('http://stackoverflow.com/questions/21285380/pandas-find-column-whose-name-contains-a-specific-string', 0.25)\n",
      "('https://en.wikipedia.org/wiki/Stratified_sampling', 0.25)\n",
      "('http://stackoverflow.com/questions/19098104/python-opencv2-cv2-wrapper-get-image-size', 0.25)\n",
      "('http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.filter.html', 0.125)\n",
      "('https://fr.wikipedia.org/wiki/Coordonn%C3%A9es_g%C3%A9ographiques', 0.125)\n",
      "('http://stackoverflow.com/questions/1267869/how-can-i-force-division-to-be-floating-point-in-python-division-keeps-rounding', 0.125)\n",
      "('http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html', 0.0625)\n",
      "('http://machinelearningmastery.com/tutorial-to-implement-k-nearest-neighbors-in-python-from-scratch/', 0.0625)\n",
      "('https://classroom.udacity.com/courses/ud810/lessons/3461518740/concepts/34384590930923', 0.0625)\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "('http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html', 1.375)\n",
      "('http://scikit-learn.org/stable/install.html#installation-instructions', 1.1875)\n",
      "('http://stackoverflow.com/questions/14661701/how-to-drop-a-list-of-rows-from-pandas-dataframe', 1.0)\n",
      "('http://scikit-learn.org/stable/auto_examples/linear_model/plot_iris_logistic.html', 0.625)\n",
      "('http://stackoverflow.com/questions/29438265/stratified-train-test-split-in-scikit-learn', 0.5)\n",
      "('https://github.com/scikit-learn/scikit-learn/issues/6161', 0.5)\n",
      "('http://scikit-learn.org/stable/tutorial/statistical_inference/supervised_learning.html', 0.3125)\n",
      "('http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html', 0.25)\n",
      "('http://hive.itcapp.com/course/status?subject=Data%20Modeling', 0.0625)\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "('http://stackoverflow.com/questions/30364255/difference-between-using-train-test-split-and-cross-val-score-in-sklearn-cross-v', 1.0)\n",
      "('http://stackoverflow.com/questions/20333092/knn-with-big-sparse-matrices-in-python', 0.5)\n",
      "('https://outlook.live.com/owa/?realm=hotmail.fr&path=/mail/search', 0.25)\n",
      "('http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html', 0.125)\n",
      "('http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html', 0.0625)\n",
      "---------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "arg2, arg3 = 'http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html', [ \"\", \"\"]\n",
    "for dataframe in df_list:\n",
    "    for i in get_weigths(dataframe, arg2, arg3):\n",
    "        print i\n",
    "    print \"---------------------------------------------------------------------------------------------------\\n\\n\"\n",
    "    print \"---------------------------------------------------------------------------------------------------\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('http://hive.itcapp.com/', 7)\n",
      "('https://github.com/', 6)\n",
      "('https://israeltechallenge.com/', 6)\n",
      "('https://trello.com/', 5)\n",
      "('http://hive.itcapp.com/#', 5)\n"
     ]
    }
   ],
   "source": [
    "list_set = [set(df.url.values)]\n",
    "for dataframe in df_list:\n",
    "    list_set.append(list(set(dataframe.url.values)))\n",
    "a = [item for sublist in list_set for item in sublist]\n",
    "b = Counter(a)\n",
    "for i in b.most_common()[:5]:\n",
    "    print i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_results_yourself(df_yourself, current_url, previous_urls):\n",
    "    best_recommandations = [i[0] for i in get_weigths(df_yourself, current_url, previous_urls)]\n",
    "    return best_recommandations[:5]\n",
    "\n",
    "\n",
    "def get_results_others(df_list, current_url, previous_urls):\n",
    "    best_recommandations = [i[0] for i in sum_score(df_list, arg2, arg3)]\n",
    "    return best_recommandations[:5]\n",
    "\n",
    "def split_website_videos(results):\n",
    "    websites = []\n",
    "    videos = []\n",
    "    for result in results:\n",
    "        if re.search(r\"^https://www.youtube.com/watch\", result):\n",
    "            videos.append(result)\n",
    "        else:\n",
    "            websites.append(result)\n",
    "    return websites, videos\n",
    "\n",
    "def interface_front_end(df_yourself, current_url, previous_urls, others=\"Results/others.txt\", yourself=\"Results/yourself.txt\", youtube=\"Results/youtube.txt\"):\n",
    "    \n",
    "    websites, videos = split_website_videos(get_results_others(df_list, current_url, previous_urls))\n",
    "    \n",
    "    file = open(others, \"w\")\n",
    "    results_others = []\n",
    "    for url in websites:\n",
    "        file.write(url)\n",
    "        file.write(os.linesep)\n",
    "    file.close()\n",
    "\n",
    "    file = open(yourself, \"w\")\n",
    "    for url in get_results_yourself(df_yourself, current_url, previous_urls):\n",
    "        file.write(url)\n",
    "        file.write(os.linesep)\n",
    "    file.close()\n",
    "\n",
    "    results_youtube = []\n",
    "    file = open(youtube, \"w\")\n",
    "    for url in videos:\n",
    "        file.write(url)\n",
    "        file.write(os.linesep)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "others = \"Results/others.txt\"\n",
    "yourself = \"Results/yourself.txt\"\n",
    "youtube = \"Results/youtube.txt\"\n",
    "\n",
    "arg2 = 'http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html'\n",
    "arg3 = [ \"\", \"\"]\n",
    "\n",
    "interface_front_end(df, arg2, arg3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
