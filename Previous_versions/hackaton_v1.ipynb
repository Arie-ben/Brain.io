{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def is_url_valid(url):\n",
    "    if url[:4] != \"http\":\n",
    "        return False\n",
    "    return True\n",
    "    \n",
    "\n",
    "def domain_without_www(domain):\n",
    "    if domain[:4] == \"www.\":\n",
    "        return domain[4:]\n",
    "    return domain\n",
    "\n",
    "\n",
    "def filter_rows(df):\n",
    "    df['is_valid'] = df.url.apply(is_url_valid)\n",
    "    df = df[df['is_valid'] == True]\n",
    "    df = df.drop(['is_valid'], axis=1)\n",
    "    df['domain'] = df.domain.apply(domain_without_www)\n",
    "    df = df.dropna()\n",
    "    df = df.sort_values('visit_time_ms', ascending=False)\n",
    "    return df.reset_index().drop('index', axis=1)\n",
    "\n",
    "\n",
    "def fuck_of_this_domain(df, sites):\n",
    "    for site in sites:\n",
    "        df = df[df['domain'] != site]\n",
    "    return df\n",
    "    \n",
    "\n",
    "def clean_date(df):   \n",
    "    df.day = [re.sub('[-:]', '/', df.day[i]) for i in range(len(df))]\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        try:\n",
    "            df.day[i] = pd.to_datetime(df.day[i] + '-'+ df.time[i])\n",
    "        except ValueError:\n",
    "            df.day[i] = None\n",
    "    return filter_rows(df.drop('time', axis=1))\n",
    "\n",
    "\n",
    "def find_indexes(df, url):\n",
    "    return df.loc[df['url'] ==  url].index.tolist()\n",
    "\n",
    "\n",
    "def next_urls(df, url):\n",
    "    indexes = [element - 1 for element in find_indexes(df, url)]\n",
    "    return db.url[indexes]\n",
    "\n",
    "\n",
    "def weighted_urls(df, url, previous_url, NUM_OF_NEXT = 5, NUM_OF_PREVIOUS = 2):\n",
    "        \n",
    "    indexes_after = [range(element - NUM_OF_NEXT, element) for element in find_indexes(df, url)]\n",
    "    indexes_before = [range(element + 1, element + 1 + NUM_OF_PREVIOUS) for element in find_indexes(df, url)]\n",
    "\n",
    "\n",
    "    weights = [float(1)/2**i for i in range(NUM_OF_NEXT)]\n",
    "    urls_before = [df.url[i].values  for i in indexes_before]\n",
    "    urls_after = [df.url[i].values  for i in indexes_after]\n",
    "    urls_weighted = []\n",
    "    \n",
    "    for j in range(len(urls_after)):\n",
    "        weights = [float(1)/2**(NUM_OF_NEXT - i -1) for i in range(NUM_OF_NEXT)]\n",
    "        coef = 1\n",
    "        i = 0\n",
    "        while i < NUM_OF_PREVIOUS and previous_url[i] == urls_before[j][i]:\n",
    "            weights = [weight*2 for weight in weights]\n",
    "            i += 1\n",
    "        urls_weighted.append([(urls_after[j][k], weights[k]) for k in range(len(weights))])            \n",
    "    return urls_weighted\n",
    "\n",
    "\n",
    "def get_weigths(df, url, previous_url):\n",
    "    weigth_dic = {}\n",
    "    for url_list in weighted_urls(df, url, previous_url):\n",
    "        for url in url_list:\n",
    "            if url[0] in weigth_dic:\n",
    "                weigth_dic[url[0]] += url[1]\n",
    "            else:\n",
    "                weigth_dic[url[0]] = url[1]\n",
    "\n",
    "    return sorted(weigth_dic.items(), key=itemgetter(1), reverse=True)\n",
    "\n",
    "\n",
    "def is_in_time(df, index, threshold):\n",
    "    if df.day[index-1] - df.day[index] < pd.Timedelta(threshold, 's') :\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filtered_domains = (['drive.google.com','google.co.il', 'localhost', 'facebook.com', 'gmail.com', 'docs.google.com', 'www.google.co.il', 'messenger.com'])\n",
    "antho = pd.read_csv('antho_secrets.csv', header=None, delimiter=\";\")\n",
    "antho.columns = (['url', 'domain', 'root domain', 'visit_time_ms', 'visit_time_str', 'day of the week', 'transition_type', 'page title'])\n",
    "df = antho[['url', 'domain', 'visit_time_ms', 'visit_time_str', 'transition_type']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://docs.python.org/2/library/datetime.html\n",
      "http://stackoverflow.com/questions/17134716/convert-dataframe-column-type-from-string-to-datetime\n",
      "https://docs.python.org/3/library/datetime.html\n"
     ]
    }
   ],
   "source": [
    "df = filter_rows(df)\n",
    "df = fuck_of_this_domain(df, filtered_domains)\n",
    "k = 100\n",
    "samples = df['url'][k:k+3].values\n",
    "for sample in samples:\n",
    "    print sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('http://stackoverflow.com/questions/16151402/python-how-can-i-check-whether-an-object-is-of-type-datetime-date',\n",
       "  4.1875),\n",
       " ('https://docs.python.org/3/library/datetime.html', 2.5),\n",
       " ('https://docs.python.org/2/library/re.html', 2.0625),\n",
       " ('https://docs.python.org/2/library/datetime.html', 1.375),\n",
       " ('https://docs.python.org/2.7/tutorial/errors.html', 1.0),\n",
       " ('http://stackoverflow.com/questions/17134716/convert-dataframe-column-type-from-string-to-datetime',\n",
       "  0.75),\n",
       " ('http://stackoverflow.com/questions/14661701/how-to-drop-a-list-of-rows-from-pandas-dataframe',\n",
       "  0.5),\n",
       " ('https://pymotw.com/2/datetime/', 0.5),\n",
       " ('http://pandas.pydata.org/pandas-docs/stable/generated/pandas.to_timedelta.html',\n",
       "  0.25),\n",
       " ('http://superuser.com/questions/244062/how-do-i-view-add-or-edit-cookies-in-google-chrome',\n",
       "  0.25),\n",
       " ('https://chrome.google.com/webstore/detail/browsing-activity-tracker/maialhkckkpdbhimboiimgdgmhlianje',\n",
       "  0.125),\n",
       " ('https://chrome.google.com/webstore/category/extensions', 0.0625)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_website = samples[0]\n",
    "previous_websites = samples[1:3]\n",
    "get_weigths(df, current_website, previous_websites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
